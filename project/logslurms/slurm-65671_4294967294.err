huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: lucien-rondier (NLI_Project). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/wandb/run-20240327_110315-5it4jsn8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-vortex-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/NLI_Project/huggingface
wandb: üöÄ View run at https://wandb.ai/NLI_Project/huggingface/runs/5it4jsn8/workspace
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
  0%|          | 0/21460 [00:00<?, ?it/s]wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.026 MB of 0.035 MB uploadedwandb: - 0.044 MB of 0.044 MB uploadedwandb: üöÄ View run peachy-vortex-9 at: https://wandb.ai/NLI_Project/huggingface/runs/5it4jsn8/workspace
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240327_110315-5it4jsn8/logs
Traceback (most recent call last):
  File "train.py", line 54, in <module>
    model = allMiniLMModel(model_name, num_labels, output_dir, train_dataset_processed, validation_dataset_processed, test_dataset_processed, batch_size, epochs, learning_rate, seed, warmup_steps,weight_decay,wandb_project_name=wandb_project_name, wandb_entity=wandb_entity, wandb_api_key=None)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/project/model.py", line 53, in __init__
    self.train()
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/project/model.py", line 98, in train
    self.trainer.train()
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3036, in training_step
    loss = self.compute_loss(model, inputs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3059, in compute_loss
    outputs = model(**inputs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1564, in forward
    outputs = self.bert(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1013, in forward
    encoder_outputs = self.encoder(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 539, in forward
    layer_output = apply_chunking_to_forward(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/pytorch_utils.py", line 237, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 552, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 466, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 201, in forward
    return F.layer_norm(
  File "/usr/users/sdi-labworks-2023-2024/sdi-labworks-2023-2024_32/NLI_project/venv/lib/python3.8/site-packages/torch/nn/functional.py", line 2546, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 10.91 GiB of which 44.38 MiB is free. Including non-PyTorch memory, this process has 10.86 GiB memory in use. Of the allocated memory 10.10 GiB is allocated by PyTorch, and 218.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
